{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2137\n",
      "toal: 27766\n",
      "clipped: 1280\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAYp0lEQVR4nO3de5RlZX3m8e8jctFABKTtQS42gc5ETEZ0tYCXiSgjctHBuCLieGmNDibBRCaaTKMmECMJZhSjEwfTSi/ACxcVYitEbJCId2gQ5abSg41029CtIBdvSeNv/jhvyZmiqnZ1dZ061VXfz1pnnb3fvfe73119up7a77v3PqkqJEmayCOG3QBJ0uxnWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFppRSW5KctgEy/81yetmrkXTK8mrk3xpmuv8QJK/nKa69k3yQJLt2vy0/ryT/EuSpdNVn2aPRw67AZpbkjzQN/to4BfAg23+9VX1pL51TwUOqKpXTMN+FwHfA7avqs1bW99M7TPJWmAhsJnez+lm4FxgeVX9EqCq/nAL6npdVV0+3jpV9X1g56m0dYz9ncqof7+qOmo66tbsY1hoWlXVr34RTeaXlwB4YVVdnuQxwLOB9wKHAK+Zzp0keeRMBanmHruhNKOSrE3yX5IcCbwFeGnrFvnmOOv/QZJbktyT5LIkT5jCPh+T5KwkG5KsT/KOvm6YVyf5UpJ3tX18L8lRfdvul+SqJPcnuTzJ+5N8pC2+qr3/uB3D0/u2G7O+iVTVvVW1EngpsDTJb7e6zk7yjja9R5LPJPlxkruTfDHJI5J8GNgX+HRry18kWZSkkrw2yfeBz/eV9f+huH+Sq5Pcl+RTSXZv+zosybpRP8sJ//36u7Vau96W5PYkG5Oc2wKRvnYsTfL9JD9M8tbJ/Jw0HIaFhqKqPgv8LXBBVe1cVU8evU6SY+n9QnoxsAD4InDeFHZ3Nr1ungOApwBHAP399IcA3wH2AP4eOCtJ2rKPAVcDjwVOBV7Zt93vtvdd2zF8dRL1daqqq4F1wH8eY/Gb2rIF9Lqv3tLbpF4JfJ/eWcrOVfX3fds8G3gi8Pxxdvkq4A+APen9nN43iTZ2/vsBr26v5wC/Qa/76x9HrfMs4D8ChwN/leSJXfvWcBgWms3+EPi7qrqldZ/8LXDQlpxdJFkIHA2cVFU/qaqNwHuA4/tWu72qPlhVDwLn0PuluTDJvsDTgL+qqn+rqi8BKyex2zHrm2ybmx8Au49R/u+tvidU1b9X1Rer+wFvp7Zj/9k4yz9cVTdW1U+AvwSOGznz2kovB86oqtuq6gHgZOD4UWc1f11VP6uqbwLfBMYKHc0ChoVmsycA721dLj8G7gYC7LWFdWwPbOir55+Ax/Wtc+fIRFX9tE3uDDweuLuvDOCOSexzvPq2xF70jne0/wWsAT6X5LYkyyZRV1eb+5ffTu/ntcekWjmxx7f6+ut+JP9/cN7ZN/1TpmnwXdPPAW4NU9dfxHcAp1XVR7diH3fQuyJrjykM7m4Adk/y6L5f+vv0LR/II5uTPI1eWDzsEtyqup9eV9Sb2pjG55NcU1VXTNCernb2H9O+9M5efgj8hN4VbSPt2o5e99dk6/0BvbDur3szcBewd8e2mmU8s9Aw3QUsSjLe5/ADwMlJngS/Gqh+SUedOybZaeTV9vE54N1Jfr0Nuu6f5Nldjauq24HVwKlJdmgD2C/sW2UT8Et6/fFbrbXvBcD5wEeq6oYx1nlBkgPaGMi99C63/WVbfNcU2/KKJAcmeTTwduATrQvtu8BOSY5Jsj3wNmDHvu26/v3OA/5Hu0hgZx4a4/CKrG2QYaFh+nh7/1GS60YvrKqLgXcC5ye5D7gR6Lqy6AHgZ32v59IbwN2B3j0M9wCfoNfvPxkvB54O/Ah4B3ABvTOVkS6m04Avty6uQydZ52ifTnI/vbOgtwJnMP5ls4uBy+kd51eB/1NVV7Zlfwe8rbXlzVuw/w/TuwjgTmAn4E+hd3UW8MfAh4D19M40+q+OmvDfD1jR6r6K3v0oPwf+ZAvapVkkfvmRNHlJLgC+XVWnDLst0kzyzEKaQJKntW6rR7R7C44F/nnIzZJmnAPc0sT+A3ARvfss1gF/VFXfGG6TpJlnN5QkqZPdUJKkTnOyG2qPPfaoRYsWDbsZkrRNufbaa39YVQvGWjYnw2LRokWsXr162M2QpG1KktvHW2Y3lCSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKnTnLyDW1OzaNklY5avPf2YGW6JpNnGMwtJUifDQpLUybCQJHUyLCRJnRzgVicHviUZFvPQeL/8JWk8dkNJkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOg0sLJLsk+TKJDcnuSnJG1v5qUnWJ7m+vY7u2+bkJGuSfCfJ8/vKj2xla5IsG1SbJUljG+TjPjYDb6qq65LsAlybZFVb9p6qelf/ykkOBI4HngQ8Hrg8yW+2xe8HngesA65JsrKqbh5g2yVJfQYWFlW1AdjQpu9Pcguw1wSbHAucX1W/AL6XZA1wcFu2pqpuA0hyflvXsJCkGTIjYxZJFgFPAb7eit6Q5FtJViTZrZXtBdzRt9m6VjZe+eh9nJBkdZLVmzZtmu5DkKR5beBhkWRn4JPASVV1H3AmsD9wEL0zj3dPx36qanlVLamqJQsWLJiOKiVJzUAfUZ5ke3pB8dGqugigqu7qW/5B4DNtdj2wT9/me7cyJiiXJM2AQV4NFeAs4JaqOqOvfM++1X4PuLFNrwSOT7Jjkv2AxcDVwDXA4iT7JdmB3iD4ykG1W5L0cIM8s3gm8ErghiTXt7K3AC9LchBQwFrg9QBVdVOSC+kNXG8GTqyqBwGSvAG4DNgOWFFVNw2w3ZKkUQZ5NdSXgIyx6NIJtjkNOG2M8ksn2k6SNFjewS1J6mRYSJI6GRaSpE6GhSSpk2EhSeo00JvyNLctWnbJmOVrTz9mhlsiadA8s5AkdfLMYo4a769+SZoKzywkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHV65KAqTrIPcC6wEChgeVW9N8nuwAXAImAtcFxV3ZMkwHuBo4GfAq+uqutaXUuBt7Wq31FV5wyq3dp6i5ZdMu6ytacfM4MtkTRdBhYWwGbgTVV1XZJdgGuTrAJeDVxRVacnWQYsA/4ncBSwuL0OAc4EDmnhcgqwhF7oXJtkZVXdM8C2bzMm+sUsSdNlYN1QVbVh5Mygqu4HbgH2Ao4FRs4MzgFe1KaPBc6tnq8BuybZE3g+sKqq7m4BsQo4clDtliQ93IyMWSRZBDwF+DqwsKo2tEV30uumgl6Q3NG32bpWNl756H2ckGR1ktWbNm2a3gOQpHlu4GGRZGfgk8BJVXVf/7KqKnpdS1utqpZX1ZKqWrJgwYLpqFKS1Aw0LJJsTy8oPlpVF7Xiu1r3Eu19YytfD+zTt/nerWy8cknSDBlYWLSrm84CbqmqM/oWrQSWtumlwKf6yl+VnkOBe1t31WXAEUl2S7IbcEQrkyTNkEFeDfVM4JXADUmub2VvAU4HLkzyWuB24Li27FJ6l82uoXfp7GsAquruJH8DXNPWe3tV3T3AdkuSRhlYWFTVl4CMs/jwMdYv4MRx6loBrJi+1kmStoR3cEuSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqdMgv/xIephFyy4Zs3zt6cfMcEskbQnPLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ0MC0lSJ8NCktTJsJAkdZpUWCR55mTKJElz02TPLP73JMskSXPQhI/7SPJ04BnAgiR/1rfo14HtBtkwSdLs0fVsqB2Andt6u/SV3wf8/qAaJUmaXSYMi6r6AvCFJGdX1e0z1CZJ0iwz2TGLHZMsT/K5JJ8feU20QZIVSTYmubGv7NQk65Nc315H9y07OcmaJN9J8vy+8iNb2Zoky7b4CCVJW22yjyj/OPAB4EPAg5Pc5mzgH4FzR5W/p6re1V+Q5EDgeOBJwOOBy5P8Zlv8fuB5wDrgmiQrq+rmSbZBkjQNJhsWm6vqzC2puKquSrJokqsfC5xfVb8AvpdkDXBwW7amqm4DSHJ+W9ewkKQZNNluqE8n+eMkeybZfeQ1xX2+Icm3WjfVbq1sL+COvnXWtbLxyh8myQlJVidZvWnTpik2TZI0lsmGxVLgz4GvANe21+op7O9MYH/gIGAD8O4p1DGmqlpeVUuqasmCBQumq1pJEpPshqqq/aZjZ1V118h0kg8Cn2mz64F9+lbdu5UxQbkkaYZMKiySvGqs8qoaPXjdVc+eVbWhzf4eMHKl1ErgY0nOoDfAvRi4GgiwOMl+9ELieOC/bck+JUlbb7ID3E/rm94JOBy4jodf6fQrSc4DDgP2SLIOOAU4LMlBQAFrgdcDVNVNSS6kN3C9GTixqh5s9bwBuIzeHeMrquqmSbZZkjRNJtsN9Sf980l2Bc7v2OZlYxSfNcH6pwGnjVF+KXDpZNopSRqMqT6i/CfAtIxjSJJmv8mOWXyaXtcR9LqDnghcOKhGaf5ZtOySMcvXnn7MDLdE0lgmO2bRf8f1ZuD2qlo3gPZIkmahyY5ZfCHJQh4a6L51cE3SWMb7y1uSZsJkvynvOHqXsr4EOA74ehIfUS5J88Rku6HeCjytqjYCJFkAXA58YlANkyTNHpO9GuoRI0HR/GgLtpUkbeMme2bx2SSXAee1+ZfivQ+SNG90fQf3AcDCqvrzJC8GntUWfRX46KAbJ0maHbrOLP4BOBmgqi4CLgJI8jtt2QsH2DZJ0izRNe6wsKpuGF3YyhYNpEWSpFmnKyx2nWDZo6axHZKkWawrLFYn+e+jC5O8jt4XIEmS5oGuMYuTgIuTvJyHwmEJsAO976OQJM0DE4ZF+2a7ZyR5DvDbrfiSqvr8wFsmSZo1JvtsqCuBKwfcFknSLOVd2JKkToaFJKmTYSFJ6mRYSJI6GRaSpE6TfeqsNBR+N7c0O3hmIUnqZFhIkjoZFpKkToaFJKmTYSFJ6mRYSJI6DSwskqxIsjHJjX1luydZleTW9r5bK0+S9yVZk+RbSZ7at83Stv6tSZYOqr2SpPEN8szibODIUWXLgCuqajFwRZsHOApY3F4nAGdCL1yAU4BDgIOBU0YCRpI0cwYWFlV1FXD3qOJjgXPa9DnAi/rKz62erwG7JtkTeD6wqqrurqp7gFU8PIAkSQM202MWC6tqQ5u+E1jYpvcC7uhbb10rG69ckjSDhjbAXVUF1HTVl+SEJKuTrN60adN0VStJYubD4q7WvUR739jK1wP79K23dysbr/xhqmp5VS2pqiULFiyY9oZL0nw202GxEhi5omkp8Km+8le1q6IOBe5t3VWXAUck2a0NbB/RyiRJM2hgT51Nch5wGLBHknX0rmo6HbgwyWuB24Hj2uqXAkcDa4CfAq8BqKq7k/wNcE1b7+1VNXrQXJI0YAMLi6p62TiLDh9j3QJOHKeeFcCKaWyaJGkLeQe3JKmTX36kbZJfiiTNLM8sJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR18qa8WWa8m80kaZg8s5AkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnQwLSVIn77PQnDLRfSp+MZI0dZ5ZSJI6GRaSpE6GhSSpk2EhSepkWEiSOhkWkqROhoUkqZNhIUnqZFhIkjoZFpKkTkMJiyRrk9yQ5Pokq1vZ7klWJbm1ve/WypPkfUnWJPlWkqcOo82SNJ8N88ziOVV1UFUtafPLgCuqajFwRZsHOApY3F4nAGfOeEslaZ6bTd1QxwLntOlzgBf1lZ9bPV8Ddk2y5xDaJ0nz1rCeOlvA55IU8E9VtRxYWFUb2vI7gYVtei/gjr5t17WyDX1lJDmB3pkH++677wCbrm3VeE+k9Wm0UrdhhcWzqmp9kscBq5J8u39hVVULkklrgbMcYMmSJVu0rSRpYkPphqqq9e19I3AxcDBw10j3Unvf2FZfD+zTt/nerUySNENmPCyS/FqSXUamgSOAG4GVwNK22lLgU216JfCqdlXUocC9fd1VkqQZMIxuqIXAxUlG9v+xqvpskmuAC5O8FrgdOK6tfylwNLAG+CnwmplvsiTNbzMeFlV1G/DkMcp/BBw+RnkBJ85A0yRJ45hNl85KkmapYV0NJc0aXlIrdfPMQpLUybCQJHUyLCRJnQwLSVInw0KS1MmwkCR1MiwkSZ28z2JIxru2X7OH919ID/HMQpLUybCQJHUyLCRJnRyzkLbQRONNjmdorvLMQpLUybCQJHUyLCRJnRyzkKaR92ZorvLMQpLUybCQJHUyLCRJnRyzkGaAYxna1nlmIUnqZFhIkjrZDSUN0ZY+qt5uKw2LZxaSpE6GhSSpk91QW2BLr2jx2/A03abzM2WXlraEYSHNU17Oqy2xzXRDJTkyyXeSrEmybNjtkaT5ZJs4s0iyHfB+4HnAOuCaJCur6ubhtkyae6bzjMOzl7ljmwgL4GBgTVXdBpDkfOBYYFaEhWMTmg+m83M+V/7PDDP0ZjqIt5Ww2Au4o29+HXBI/wpJTgBOaLMPJPnOVuxvD+CHW7H9tsrjnl887q2Ud05HLdNrgjZN5rifMN6CbSUsOlXVcmD5dNSVZHVVLZmOurYlHvf84nHPL1t73NvKAPd6YJ+++b1bmSRpBmwrYXENsDjJfkl2AI4HVg65TZI0b2wT3VBVtTnJG4DLgO2AFVV10wB3OS3dWdsgj3t+8bjnl6067lTVdDVEkjRHbSvdUJKkITIsJEmdDIs+8+mRIklWJNmY5Ma+st2TrEpya3vfbZhtnG5J9klyZZKbk9yU5I2tfK4f905Jrk7yzXbcf93K90vy9fZ5v6BdPDLnJNkuyTeSfKbNz5fjXpvkhiTXJ1ndyqb8WTcsmr5HihwFHAi8LMmBw23VQJ0NHDmqbBlwRVUtBq5o83PJZuBNVXUgcChwYvs3nuvH/QvguVX1ZOAg4MgkhwLvBN5TVQcA9wCvHV4TB+qNwC198/PluAGeU1UH9d1fMeXPumHxkF89UqSq/g0YeaTInFRVVwF3jyo+FjinTZ8DvGgm2zRoVbWhqq5r0/fT+wWyF3P/uKuqHmiz27dXAc8FPtHK59xxAyTZGzgG+FCbD/PguCcw5c+6YfGQsR4psteQ2jIsC6tqQ5u+E1g4zMYMUpJFwFOArzMPjrt1xVwPbARWAf8X+HFVbW6rzNXP+z8AfwH8ss0/lvlx3ND7g+BzSa5tj0OCrfisbxP3WWjmVVUlmZPXVSfZGfgkcFJV3df7Y7Nnrh53VT0IHJRkV+Bi4LeG26LBS/ICYGNVXZvksCE3ZxieVVXrkzwOWJXk2/0Lt/Sz7pnFQ3ykCNyVZE+A9r5xyO2Zdkm2pxcUH62qi1rxnD/uEVX1Y+BK4OnArklG/mCci5/3ZwL/Nclaet3KzwXey9w/bgCqan1730jvD4SD2YrPumHxEB8p0jvepW16KfCpIbZl2rX+6rOAW6rqjL5Fc/24F7QzCpI8it73wtxCLzR+v6025467qk6uqr2rahG9/8+fr6qXM8ePGyDJryXZZWQaOAK4ka34rHsHd58kR9Pr4xx5pMhpw23R4CQ5DziM3mOL7wJOAf4ZuBDYF7gdOK6qRg+Cb7OSPAv4InADD/Vhv4XeuMVcPu7/RG8wczt6fyBeWFVvT/Ib9P7i3h34BvCKqvrF8Fo6OK0b6s1V9YL5cNztGC9us48EPlZVpyV5LFP8rBsWkqROdkNJkjoZFpKkToaFJKmTYSFJ6mRYSJI6GRbSFCR5T5KT+uYvS/Khvvl3J/mzKdR72MjTUaXZxLCQpubLwDMAkjyC3v0qT+pb/gzgK12VtKcdS7OeYSFNzVfoPTIDeiFxI3B/kt2S7Ag8EXhM+x6FG9r3h+wIv/qegXcmuQ54SfselW+3+ReP7CDJs9t3EVzf6tllZg9ReogPEpSmoKp+kGRzkn3pnUV8ld7TS58O3AvcSu+x2IdX1XeTnAv8Eb0nBAD8qKqemmSntu5zgTXABX27eTNwYlV9uT388OczcGjSmDyzkKbuK/SCYiQsvto3vw74XlV9t617DvC7fduOhMJvtfVurd7jFD7St86XgTOS/Cmwa99jtaUZZ1hIUzcybvE79LqhvkbvzOIZwL92bPuTrsqr6nTgdcCjgC8nmfOPFdfsZVhIU/cV4AXA3VX1YHsg2670AuOTwKIkB7R1Xwl8YYw6vt3W27/Nv2xkQZL9q+qGqnonvaciGxYaGsNCmrob6F0F9bVRZfdW1TrgNcDHk4w85fYDoyuoqp8DJwCXtAHu/u8XOCnJjUm+Bfw78C+DOQypm0+dlSR18sxCktTJsJAkdTIsJEmdDAtJUifDQpLUybCQJHUyLCRJnf4fNZtj8hh+JY0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import readers\n",
    "from icecream import ic\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "temp_dir = 'data/'\n",
    "\n",
    "#### Anecdotes\n",
    "corpus = readers.ScruplesCorpus(data_dir=temp_dir)\n",
    "\n",
    "\n",
    "\n",
    "corpus2 = readers.ScruplesCorpusDataset(\n",
    "            data_dir=temp_dir,\n",
    "            split='train',\n",
    "            transform=None,\n",
    "            label_transform=None,\n",
    "            label_scores_transform=None)\n",
    "\n",
    "\n",
    "#### Graph Out Word Distributions ####\n",
    "\n",
    "\n",
    "#### Graph out text body distribution ####\n",
    "fig,ax = plt.subplots(1,1)\n",
    "\n",
    "#print(corpus.train[1][\"text\"].astype(str).apply(len).max())\n",
    "text_len=np.zeros(corpus.train[1][\"text\"].shape,dtype=np.int32)\n",
    "for i,title in enumerate(corpus.train[1][\"text\"]):\n",
    "    text_len[i] =int(len(title.split(\" \")))\n",
    "\n",
    "print(np.max(text_len))\n",
    "print(\"toal:\",len(text_len))\n",
    "print(\"clipped:\",len(text_len[text_len > 750]))\n",
    "ax.hist(text_len, bins = range(0,2200))\n",
    "ax.set_title(\"Text Body Distribution\")\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#### Graph out title distribution ####\n",
    "fig,ax = plt.subplots(1,1)\n",
    "#print(corpus.train[1][\"title\"].astype(str).apply(len).mean())\n",
    "lens=np.zeros(corpus.train[1][\"title\"].shape,dtype=np.int32)\n",
    "for i,title in enumerate(corpus.train[1][\"title\"]):\n",
    "    lens[i] =int(len(title.split(\" \")))\n",
    "#print(lens)\n",
    "\n",
    "#print(np.mean(lens))\n",
    "ax.hist(lens, bins = range(0,50))\n",
    "# ax.set_xticks(range(0,50))\n",
    "ax.set_title(\"Title Length Distribution\")\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  title  \\\n",
      "0                    AITA for wanting my father to die?   \n",
      "1                         AITA for forgetting to flush?   \n",
      "2               AITA Dinner plans and who's responsible   \n",
      "3     AITA for being upset about my SO using our ope...   \n",
      "4              AITA for wanting to get rid of my puppy?   \n",
      "...                                                 ...   \n",
      "2495    AITA For cutting a toxic friend out of my life?   \n",
      "2496              AITA Because my friend feels awkward?   \n",
      "2497  AITA for leaving heartbroken a guy who had a c...   \n",
      "2498  AITA - My husband and I are struggling in our ...   \n",
      "2499     AITA for spending more time with other people?   \n",
      "\n",
      "                                                   text  \n",
      "0        Throwaway for obvious reasons.\\n\\n   I drop...  \n",
      "1     For background, my mom's cousin is staying wit...  \n",
      "2     AITA Dinner plans and who's responsible\\n\\n&am...  \n",
      "3     Background: I can't give much detail because o...  \n",
      "4     My girlfriend volunteers at an animal shelter ...  \n",
      "...                                                 ...  \n",
      "2495  Okay this is a long 2 part one so strap in and...  \n",
      "2496  Apologies for poor formatting - posting from m...  \n",
      "2497  First of all, I should precise that I am a str...  \n",
      "2498  My husband and I have been married for two yea...  \n",
      "2499  I know the title sounds weird and quite vague,...  \n",
      "\n",
      "[2500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(corpus.test[1])\n",
    "\n",
    "test = corpus.test[1]\n",
    "xtest = test['text']\n",
    "ytest = test['title']\n",
    "_, _, pairs_test = prepareData( xtest, ytest , False)\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs_test)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "tensor([[-1.1559, -0.9408, -1.2210],\n",
      "        [-1.1377, -0.9563, -1.2203],\n",
      "        [-1.1656, -0.9659, -1.1790],\n",
      "        [-1.2338, -0.8290, -1.3008],\n",
      "        [-1.2095, -0.8931, -1.2301]])\n",
      "tensor([[-1.1159, -0.9009, -1.3236],\n",
      "        [-1.0990, -0.9133, -1.3259],\n",
      "        [-1.1297, -0.9042, -1.3020],\n",
      "        [-1.1702, -0.7882, -1.4480],\n",
      "        [-1.1695, -0.8135, -1.4017]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    # Tags are: DET - determiner; NN - noun; V - verb\n",
    "    # For example, the word \"The\" is a determiner\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "word_to_ix = {}\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "print(word_to_ix)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}  # Assign each tag with a unique index\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(10):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AITA for hiding my controller?\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "pred = 'AITA for eating ass?'\n",
    "# dataset = \n",
    "# dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "print(corpus.train[1]['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7531de924fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1164\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2996\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "loss = criterion(pred, corpus.train[1]['title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_size=400, hidden_size=512, vocab_size=1500, caption_max_len=30, device=\"cpu\"):\n",
    "        super(encoder, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.caption_max_len = caption_max_len\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.decoder_input_size = 2 * self.hidden_size  # output of image encoder + word embedding size\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_f2f = nn.Linear(self.image_feature_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "        self.lstm_v2h = nn.LSTM(self.hidden_size, self.hidden_size, batch_first=True)\n",
    "        self.lstm_h2c = nn.LSTM(self.decoder_input_size, self.hidden_size, batch_first=True)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
    "        self.linear_h2c = nn.Linear(self.hidden_size, self.vocab_size)\n",
    "\n",
    "    # caption for training, or index of <BOS> for inference\n",
    "    def forward(self, video_features, caption=None, word2idx=None):\n",
    "        # Video features -> hidden layer\n",
    "        video_features = self.relu(video_features)\n",
    "        video_features = self.dropout(video_features)\n",
    "        video_features = self.linear_f2f(video_features)\n",
    "        batch_size, num_video_features = video_features.size(0), video_features.size(1)\n",
    "        video_padding = torch.zeros((batch_size, self.caption_max_len - 1, self.hidden_size), device=self.device)\n",
    "        padded_video_features = torch.cat([video_features, video_padding], dim=1)\n",
    "        video_out, video_hidden = self.lstm_v2h(padded_video_features)\n",
    "\n",
    "        if self.training:\n",
    "            # Encoded videos feature -> caption decoder\n",
    "            # caption input is from <BOS> to seq length -1\n",
    "            embedded_caption = self.embedding(caption[:, :-1])          # size: (B, 31, 512)\n",
    "            caption_padding = torch.zeros((batch_size, num_video_features, self.hidden_size), device=self.device)\n",
    "            # Append empty embedding state (video part) before captions\n",
    "            decoder_input = torch.cat([caption_padding, embedded_caption], dim=1)\n",
    "            # Stack video feature and word embedding\n",
    "            decoder_input = torch.cat([video_out, decoder_input], dim=2)\n",
    "            caption_out, _ = self.lstm_h2c(decoder_input)\n",
    "            caption_out = caption_out[:, num_video_features:]\n",
    "            caption_out = self.dropout(caption_out)\n",
    "            caption_out = self.linear_h2c(caption_out)\n",
    "            caption_out = torch.permute(caption_out, (0, 2, 1))  # To follow cross entropy loss: (B, C, k...) format\n",
    "            return caption_out\n",
    "        else:\n",
    "            caption_padding = torch.zeros((batch_size, num_video_features, self.hidden_size), device=self.device)\n",
    "            vid_feature_input = torch.cat([video_out[:, :num_video_features, :], caption_padding], dim=2)\n",
    "            # Video features -> captions\n",
    "            caption_out, caption_hidden = self.lstm_h2c(vid_feature_input)\n",
    "            # feeds <BOS> into LSTM\n",
    "            bos_tensor = torch.full([batch_size, 1], word2idx[\"<BOS>\"], device=self.device)\n",
    "            results = bos_tensor.clone()\n",
    "\n",
    "            bos_tensor = self.embedding(bos_tensor)\n",
    "            bos_tensor = torch.cat([video_out[:, num_video_features, :].unsqueeze(dim=1), bos_tensor], dim=2)\n",
    "            next_word, next_hidden = self.lstm_h2c(bos_tensor, caption_hidden)\n",
    "\n",
    "            for idx in range(self.caption_max_len - 2):\n",
    "                next_word = self.linear_h2c(next_word)\n",
    "                word_out = next_word.max(dim=2)[1]\n",
    "                results = torch.hstack([results, word_out])\n",
    "                word_out = self.embedding(word_out)\n",
    "\n",
    "                # concat previous word embedding with video features\n",
    "                next_word = torch.cat([video_out[:, num_video_features+idx+1, :].unsqueeze(dim=1), word_out], dim=2)\n",
    "                next_word, next_hidden = self.lstm_h2c(next_word, next_hidden)\n",
    "\n",
    "            return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'sequence_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f5d6ba5a7cd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Knock knock. Whos there?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-f5d6ba5a7cd9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, batch_s, seq_len, max_epoch)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/my-env/lib/python3.7/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_source\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-f5d6ba5a7cd9>\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords_indexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'sequence_length'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        args,\n",
    "    ):\n",
    "        self.args = args\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv('data/reddit-cleanjokes.txt')\n",
    "        text = train_df['Joke'].str.cat(sep=' ')\n",
    "        return text.split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.args.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+self.args.sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+self.args.sequence_length+1]),\n",
    "        )\n",
    "    \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))\n",
    "\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train(dataset, model, args):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(args.max_epochs):\n",
    "        state_h, state_c = model.init_state(args.sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--max-epochs', type=int, default=10)\n",
    "parser.add_argument('--batch-size', type=int, default=256)\n",
    "parser.add_argument('--sequence-length', type=int, default=4)\n",
    "args = parser.parse_args()\n",
    "dataset = Dataset('./272/project/CS272-Final-Project/data/reddit-cleanjokes.txt')\n",
    "model = Model(dataset)\n",
    "\n",
    "train(dataset, model, args)\n",
    "print(predict(dataset, model, text='Knock knock. Whos there?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3e923e10985c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "            \n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
